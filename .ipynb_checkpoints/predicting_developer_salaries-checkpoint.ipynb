{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c8d811",
   "metadata": {},
   "source": [
    "## Predicting Developer Salaries\n",
    "\n",
    "### Project Background\n",
    "Africa’s Tech sector has become one of the fastest growing tech ecosystems in the world with tech being one of the fastest growing sectors in Africa. This has led to a rise in demand for jobs in the industry. \n",
    "However, unlike other parts of the world, information on remuneration in these jobs remains hard to come by. Existing resources such as glassdoor and brighter monday have limited information on salaries in Africa. \n",
    "Over the past few years, it has been observed that foreign companies enter the African Market, offering more competitive salaries compared to local companies resulting in mass movement of experienced developers into these new roles.\n",
    "This project seeks to solve this problem by developing a platform that can predict developer salaries based on their personal information, and also, providing comparison between different incomes in different regions for similar roles.\n",
    "\n",
    "\n",
    "### Stakeholders: \n",
    "- Jobseekers\n",
    "- Employers\n",
    "- Recruitment agencies\n",
    "\n",
    "### Business Understanding\n",
    "Salary negotiation can be a critical stage in the job search process, and job seekers often encounter various challenges during this phase like lack of information on salary trends. This means that a jobseeker might spend valuable time researching industry salary trends. Some might not be so lucky as the information might be non existent.\n",
    "\n",
    "As the Tech labour market becomes more competitive, offering the right salary for new and current employees is crucial for employers as it means keeping or losing a valued resource. Thus it is imperative for them to offer fair and competitive compensation that is benchmarked to their industry\n",
    "\n",
    "Our project looks at coming up with salary prediction model to help both jobseekers and employers with the above challenges. We will focus on the tech industry (developers) and use data from stackoverflow's annual developer survey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc3234",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "Our solution to the problem of inadequate salary data for both employees and employers is to develop a salary prediction model, to estimate salaries based on relevant job specifications. The model will assist in making informed decisions related to compensation and provide valuable insights for both job seekers and employers.\n",
    "\n",
    "The salary prediction model will enable job seekers to have a better understanding of the salary expectations associated with their qualifications and experience. \n",
    "\n",
    "Employers can use the model to make informed decisions regarding fair compensation packages for new hires or salary adjustments for existing employees.\n",
    "\n",
    "Job sites like linkedin, glassdoor, brigther monday can use this model for jobs displayed on the sites by quoting the estimated salaries \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e63de",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "- The main objective of this project is to come up with a salary rediction model that will:\n",
    "- Enable Jobseekers to ask for competitive salaries during contract negotiations.\n",
    "- Assist employers in offering fair compensation to their employees.\n",
    "- Assist Recruitment agencies offer accurate salary estimates to their clients.\n",
    "\n",
    "These objectives will be achieved through the following specific objectives:\n",
    "- To select the most important features in the dataset to be used in Salary prediction.\n",
    "- To describe how features such as Proffessional experience and Education level affect Annual compensation.\n",
    "- To build multiple regression models and identify the most suitable model to be used in the prediction.\n",
    "- To deploy the model using streamlit as an online dashboard.\n",
    "\n",
    "### Success Metrics\n",
    "\n",
    "The metrics to be used to measure the success of the model are:\n",
    "- Mean Absolute Error\n",
    "- Root Mean Square Error\n",
    "- Rsquared\n",
    "\n",
    "An Rsquared  value of 75% or more will be considered a success, i.e the model explains more than 75% of the  variance in pay of the developers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a42f9",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "The data comes from [stakoverflow annual developer survey](https://insights.stackoverflow.com/survey/) for 2022. Each row shows the responses given by a developer. It has 73268 rows and  79 columns. The data has missing values, but no duplicate rows.\n",
    "\n",
    "The target variable ConvertedCompYearly shows the annual salary for each developer.\n",
    "The data contains responses from 180 countries.\n",
    "Opportunities to clean the data and use PCA to reduce the number of columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "39033c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('dark')\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a68cf517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "df = pd.read_csv(r'C:\\Users\\ADMIN\\OneDrive - Kantar\\XXXXXXXX\\POV1\\Data Science\\Moringa\\data\\survey_results_public.csv')\n",
    "df.head(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f3d54",
   "metadata": {},
   "source": [
    "#### Class for Understanding Dataset\n",
    "Below we create a class to:\n",
    "- give data dimensions\n",
    "- display column info\n",
    "- give descriptive stats on numerical columns\n",
    "- check data types, duplicates & missing values\n",
    "\n",
    "From the below outputs of the class:\n",
    "- The Dataset has no duplicates\n",
    "- Data has 6 numeric columns and 73 categorical columns\n",
    "- 77 columns have missing values, ranging from 2% (`country` column) to 100% (`VCHostingProfessional use` & `VCHostingPersonal use` columns). We will deal with missing values in the data cleaning section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2c9b7840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to describe dataset\n",
    "\n",
    "class Describer:\n",
    "    \n",
    "    # initialize object\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    # method to check shape of data\n",
    "    def shape(self):\n",
    "        out = print(f\"The DataFrame has:\\n\\t* {self.df.shape[0]} rows\\n\\t* {self.df.shape[1]} columns\", '\\n')\n",
    "        return out\n",
    "    \n",
    "    # method to check info on dataset\n",
    "    def data_info(self):\n",
    "        out = print(self.df.info(), '\\n')\n",
    "        return out\n",
    "    \n",
    "    # method to describe numerical columns\n",
    "    def data_describe(self):\n",
    "        out = self.df.describe()\n",
    "        return out\n",
    "    \n",
    "    # method to check data types\n",
    "    def data_type(self):\n",
    "        \"\"\"A simple function to check the data types on th datasets \"\"\"\n",
    "\n",
    "        print(\"Data has\",len( df.select_dtypes(include='number').columns),\n",
    "                \"Numeric columns\")\n",
    "    \n",
    "        print(\"and\", len(df.select_dtypes(include='object').columns),\n",
    "          \"Categorical columns\")\n",
    "\n",
    "        print('*******************')\n",
    "        print('*******************')\n",
    "\n",
    "        print('Numerical Columns:', df.select_dtypes(include='number').columns)\n",
    "        print('Categorical Coulumns:', df.select_dtypes(include='object').columns)\n",
    "\n",
    "        return None\n",
    "    \n",
    "    # check duplicates \n",
    "\n",
    "    def check_duplicates(self):\n",
    "        duplicates = []\n",
    "\n",
    "        \"\"\"Function that iterates through the rows of our dataset to check whether they are duplicated or not\"\"\"\n",
    "        \n",
    "        for i in df.duplicated():\n",
    "            duplicates.append(i)\n",
    "        duplicates_set = set(duplicates)\n",
    "        if(len(duplicates_set) == 1):\n",
    "            print('The Dataset has No Duplicates')\n",
    "\n",
    "        else:\n",
    "            duplicates_percentage = np.round(((sum(duplicates)/len(df)) * 100 ), 2)\n",
    "            print(f'Duplicated rows constitute of {duplicates_percentage} % of our dataset')\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # method to check missing values\n",
    "    def missing_values(self):\n",
    "\n",
    "        \"\"\" Function for checking null values in percentage in relation to length of the dataset \"\"\"\n",
    "\n",
    "        if df.isnull().any().any() == False :\n",
    "\n",
    "            print(\"There Are No Missing Values\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            missing_values = df.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "            missing_val_percent = ((df.isnull().sum()/len(df)).sort_values(ascending=False))\n",
    "\n",
    "            missing_df = pd.DataFrame({'Missing Values': missing_values, 'Percentage %': missing_val_percent})\n",
    "\n",
    "            return missing_df[missing_df['Percentage %'] > 0]\n",
    "        \n",
    "# creating an instance of the class describer\n",
    "describe_df = Describer(df)\n",
    "\n",
    "# lets view the shape of the data\n",
    "describe_df.shape()\n",
    "\n",
    "# lets check for duplicates\n",
    "describe_df.check_duplicates()\n",
    "\n",
    "# lets describe data types\n",
    "describe_df.data_type()\n",
    "\n",
    "# lets view the info of the data\n",
    "describe_df.data_info()\n",
    "\n",
    "# lets describe numerical cols\n",
    "describe_df.data_describe()\n",
    "\n",
    "# lets get missing values\n",
    "describe_df.missing_values()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee4330",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "The data cleaning process will entail the following:\n",
    "- Renaming clumsily worded columns for easier understanding\n",
    "- Dealing with missing data from the 77 columns\n",
    "    - Drop columns with 100% missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c09f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9ba0c0e",
   "metadata": {},
   "source": [
    "#### Renaming Columns\n",
    "When going through the data, we noticed some clumsily worded columns. To make the data easier to understand, we came up with new names. Below we code a function that will take in the new and old names as key-value pairs, then use these to rename the columns in the dataframe.\n",
    "\n",
    "We have run the function below and confirmed renaming of a few columns e.g. columns 3 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "163ea491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rename(data):\n",
    "\n",
    "    # import text doc with old and new col names\n",
    "    cols = pd.read_csv(r'cols.txt', sep='\\t')\n",
    "\n",
    "    # create a dictionary with the cols df\n",
    "    cols_dict = dict(zip(cols['Old'], cols['New']))\n",
    "\n",
    "    # rename cols in the dataframe\n",
    "    df = data.rename(columns = cols_dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = rename(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0687f8",
   "metadata": {},
   "source": [
    "### Cleaning Missing Data\n",
    "`VCHostingProfessional use` and `VCHostingPersonal use` columns have no data, hence have 100% missing values. We drop these 2 columns for this reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "71781d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_df.missing_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f3cef514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to drop 'VCHostingProfessional use' &  'VCHostingPersonal use' cols\n",
    "def dropping_columns(data, columns):\n",
    "\n",
    "    \"\"\"A simple function to drop columns with missing values\"\"\"\n",
    "\n",
    "    drop_column = data.drop(columns=columns, axis=1)\n",
    "    \n",
    "    return drop_column\n",
    "\n",
    "columns_to_drop = ['VCHostingProfessional use', 'VCHostingPersonal use']\n",
    "\n",
    "df = dropping_columns(df, columns_to_drop)\n",
    "\n",
    "describe_df.missing_values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d223e2b",
   "metadata": {},
   "source": [
    "### Columns With Legitimately Missing Values\n",
    "The column `Participation_PDS` allowed a developer to choose wether or not they would answer the subsequent 20 questions. This means that the missing values for these will be missing because they should. \n",
    "\n",
    "In `Participation_PDS` we will replace NA with No, then in the subsequent PDS questions, replace NA with Not Answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "551e4f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace NA in Participation_PDS with No\n",
    "def fill_pds(data):\n",
    "    data['Participation_PDS'] = data.Participation_PDS.fillna('No')\n",
    "    return data\n",
    "\n",
    "df = fill_pds(df)\n",
    "df.Participation_PDS.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "e498e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean cols 'Contributor_or_Manager':'Learning_Support'\n",
    "def replace_na(data):\n",
    "    for index, row in data.iterrows():\n",
    "        if row['Participation_PDS'] == 'No':\n",
    "            data.loc[index, 'Contributor_or_Manager':'Learning_Support'] = 'Not Answered'\n",
    "    return data\n",
    "\n",
    "df = replace_na(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "3af8b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_df.missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f15a0",
   "metadata": {},
   "source": [
    "After cleaning the legitimate missing columns above, we expected subsequent values to contain no missing data. However, on inspecting some 'Yes' values in  `Participation_PDS` we realised they also had some missing values in the subsequent columns. Below we clean these missing values by assigning them to 'Not Answered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "208254ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace yes in Participation_PDS with missing vals in subsequent columns\n",
    "    # Define the range of columns\n",
    "columns_range = ['Contributor_or_Manager', 'WorkExp', 'Extroverted', 'Siloed_Teams',\n",
    "       'Information_Availability', 'Well_Resourced', 'Info_Answ_Resource',\n",
    "       'Recurring_Responses', 'Workflow_Interruptions',\n",
    "       'External_Team_Assistance', 'External_Team_Interaction',\n",
    "       'Knowledge_Silos', 'TimeSearching_Answers', 'TimeAnswering',\n",
    "       'Onboarding_Duration', 'ProfessionalTech', 'Is_Involved_Onboarding',\n",
    "       'Employer_Learning_Resources', 'Learning_Support']\n",
    "\n",
    "def replace_yesna(data, columns_range, valuefill):\n",
    "    # Replace 'NA' with 'not answered' in the specified range of columns\n",
    "    data[columns_range] = data[columns_range].fillna(valuefill)\n",
    "    return data\n",
    "\n",
    "df = replace_yesna(df, columns_range,'Not Answered')\n",
    "\n",
    "df.TimeSearching_Answers.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73913aa",
   "metadata": {},
   "source": [
    "After cleaning these columns, we now have 55 columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "1408ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(describe_df.missing_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347beeb",
   "metadata": {},
   "source": [
    "To replace the missing values of `SurveyLength` and `SurveyEase`, we reviewed the most common values in both columns. In the former, 'Appropriate in length' is 76% while in the latter, 'Easy' is 67%. Below we use these 2 values to fill missing values for the 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e10360c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for \n",
    "print(df.SurveyLength.value_counts(normalize=True))\n",
    "print(df.SurveyEase.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "86749ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_eas(data):\n",
    "    data.SurveyLength.fillna('Appropriate in length', inplace=True)\n",
    "    data.SurveyEase.fillna('Easy', inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "df = len_eas(df)\n",
    "\n",
    "print(df.SurveyLength.isna().sum())\n",
    "print(df.SurveyEase.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e856c",
   "metadata": {},
   "source": [
    "The `Annual_Salary` column is our target variable. It has 47% of its values missing. **to be continued**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "55ad6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Annual_Salary'].isna().sum())\n",
    "print(df['Total_Salary'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "375edbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(describe_df.missing_values()))\n",
    "describe_df.missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9e753",
   "metadata": {},
   "source": [
    "Below we select rows with 20% - 70% missing values in all columns. Looking at the sample distribution of missing values, we will drop all rows with more than 50% missing values as they quality of their responses are doubful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "1ea2490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate 20% - 70% of col missing values\n",
    "def missing_vals(data):\n",
    "    # Calculate the percentage of missing values for each row\n",
    "    missing_percentages = data.isnull().mean(axis=1) * 100\n",
    "\n",
    "    # Select rows with 20% missing values\n",
    "    rows_20_percent_missing = data[missing_percentages >= 20]\n",
    "\n",
    "    # Select rows with 50% missing values\n",
    "    rows_50_percent_missing = data[missing_percentages >= 50]\n",
    "\n",
    "    # Select rows with 60% missing values\n",
    "    rows_60_percent_missing = data[missing_percentages >= 60]\n",
    "\n",
    "    # Select rows with 70% missing values\n",
    "    rows_70_percent_missing = data[missing_percentages >= 70]\n",
    "\n",
    "    out = print(\"20% missing: \", len(rows_20_percent_missing), \"50% missing: \", len(rows_50_percent_missing), \n",
    "                \"60% missing: \", len(rows_60_percent_missing), \"70% missing: \", len(rows_70_percent_missing))\n",
    "    return out\n",
    "\n",
    "missing_vals(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "fef1f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to drop rows with more than 50% missing values\n",
    "def drop_50(data):\n",
    "    threshold = len(data.columns) * 0.5  # 50% of total columns\n",
    "    data = data.dropna(thresh=threshold)\n",
    "\n",
    "    return data\n",
    "\n",
    "df = drop_50(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "b14808b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = \"\"\"Code_Certifications\n",
    "Proj_Mgmt_WantToWorkWith\n",
    "MiscTechWantToWorkWith\n",
    "Annual_Salary\n",
    "Total_Salary\n",
    "PlatformWantToWorkWith\n",
    "Salary_Frequency\n",
    "MiscTechHaveWorkedWith\n",
    "WebframeWantToWorkWith\n",
    "Proj_Mgmt_HVWorkedWith\n",
    "ToolsTechWantToWorkWith\n",
    "Proj_Mgmt_SyncWantToWorkWith\n",
    "PlatformHaveWorkedWith\n",
    "Learn_Code_Source\n",
    "PurchaseInfluence\n",
    "DatabaseWantToWorkWith\n",
    "OrgSize\n",
    "Currency\n",
    "Pro_Experience\n",
    "WebframeHaveWorkedWith\n",
    "ToolsTechHaveWorkedWith\n",
    "SOA_ParticipationFreq\n",
    "Outofwork_Coding\n",
    "Remote_vs_Onsite\n",
    "DatabaseHaveWorkedWith\n",
    "DevType\n",
    "Proj_Mgmt_SyncHaveWorkedWith\n",
    "NEWCollabToolsWantToWorkWith\n",
    "OpSysProfessional use\n",
    "MentalHealth\n",
    "Sexuality\n",
    "LanguageWantToWorkWith\n",
    "Disability\n",
    "Purchase_Research\n",
    "VCInteraction\n",
    "Ethnicity\n",
    "Trans\n",
    "NEWCollabToolsHaveWorkedWith\n",
    "Gender\n",
    "Age\n",
    "OpSysPersonal use\n",
    "SOVisitFreq\n",
    "LanguageHaveWorkedWith\n",
    "Blockchain_Sentiment\n",
    "Coding_Experience\n",
    "New_Stack_Overfl_Sites\n",
    "VersionControlSystem\n",
    "Belong_SO_Comm\n",
    "Education_Level\n",
    "SOAccount_Ownership\n",
    "Learn_Code_Method\n",
    "Employment_Status\n",
    "\"\"\"\n",
    "# function to split cols variable into a list\n",
    "def listcols(cols):    \n",
    "    cols_list = cols.split('\\n')\n",
    "    cols_list = [col.strip() for col in cols_list if col.strip()]\n",
    "\n",
    "    return cols_list\n",
    "\n",
    "cols_list = listcols(cols)\n",
    "print(cols_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8d690",
   "metadata": {},
   "source": [
    "We select the columns with missing values, then below we run a profiles report to inspect the properties of each columns, and decide on how to deal with the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "b769fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display profile report for cols \n",
    "# with missing values\n",
    "def profile_report(data, cols_list):\n",
    "    missing_cols = data[cols_list]\n",
    "    profile = ProfileReport(missing_cols, title = \"Profiling Report\", minimal = True)\n",
    "    return profile\n",
    "\n",
    "#profile_report(df, cols_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a398b07",
   "metadata": {},
   "source": [
    "#### Drop `Total_Salary`, `Salary_Frequency` and `Currency` Columns\n",
    "These columns are used to compute `Annual_Salary` column, which is our target variable. Thus they should not form part of the feature variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "4c6c8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our function to drop\n",
    "columns_to_drop1 = ['Total_Salary', 'Salary_Frequency', 'Currency']\n",
    "df = dropping_columns(df, columns_to_drop1)\n",
    "df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "8914c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming cols have been dropped\n",
    "def confirm(data, columns_to_drop1):    \n",
    "    for i in columns_to_drop1:\n",
    "        print(i in data.columns)\n",
    "    return None\n",
    "\n",
    "confirm(df, columns_to_drop1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c407846",
   "metadata": {},
   "source": [
    "#### Handle Missing Values with None of the Above\n",
    "The columns below have valid missing values because the response options did not allow the developers specify the responses that did not apply to them. For example in the column `Code_Certifications` asks developers the online resources they used to learn coding. However, it is possible that some people didn't use these resources because they learned coding in the degree courses for example. Since this option misses from the responses, those develpers opted to skip for this reason. The same logic applies to the remaining columns selected below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "179cd819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of columns\n",
    "cols1 = \"\"\"Code_Certifications\n",
    "Proj_Mgmt_WantToWorkWith\n",
    "MiscTechWantToWorkWith\n",
    "PlatformWantToWorkWith\n",
    "MiscTechHaveWorkedWith\n",
    "WebframeWantToWorkWith\n",
    "Proj_Mgmt_HVWorkedWith\n",
    "ToolsTechWantToWorkWith\n",
    "Proj_Mgmt_SyncWantToWorkWith\n",
    "PlatformHaveWorkedWith\n",
    "Learn_Code_Source\n",
    "PurchaseInfluence\n",
    "DatabaseWantToWorkWith\n",
    "WebframeHaveWorkedWith\n",
    "ToolsTechHaveWorkedWith\n",
    "Outofwork_Coding\n",
    "DevType\n",
    "Proj_Mgmt_SyncHaveWorkedWith\n",
    "NEWCollabToolsWantToWorkWith\n",
    "MentalHealth\n",
    "LanguageWantToWorkWith\n",
    "Disability\n",
    "Purchase_Research\n",
    "VCInteraction\n",
    "NEWCollabToolsHaveWorkedWith\n",
    "New_Stack_Overfl_Sites\n",
    "OrgSize\n",
    "Remote_vs_Onsite\n",
    "DatabaseHaveWorkedWith\"\"\"\n",
    "\n",
    "cols_list1 = listcols(cols1)\n",
    "print(cols_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "3e2a73b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use function to filll in list of columns with None\n",
    "df = replace_yesna(df, cols_list1,'None of the Above')\n",
    "df.Code_Certifications.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23349cf0",
   "metadata": {},
   "source": [
    "#### Missing Values filled With One of the Response Options\n",
    "For this set of columns, we have identified a criteria to fill in the missing values based on the response distribution. For example:\n",
    "- demographic variable missing values are filled with preferred not to say option due to sensitivity\n",
    "- some other variables filled with the most common response option based on domain knowledge e.g operating systems used\n",
    "- while some filled with 'not sure/can't rememember'- e.g. blockchain sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "05ee6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename values in the 'Experience' column\n",
    "def rename_values(df, column):\n",
    "    df[column] = df[column].replace({'Less than 1 year': 0.5, 'More than 50 years': 50})\n",
    "    return df\n",
    "\n",
    "# Call the function to rename values in the 'Pro_Experience' column\n",
    "df = rename_values(df, 'Pro_Experience')\n",
    "\n",
    "# Call the function to rename values in the 'Coding_Experience' column\n",
    "df = rename_values(df, 'Coding_Experience')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "553bd743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols to fill with 'Prefer not to say'\n",
    "demo = ['Sexuality', 'Ethnicity', 'Trans', 'Gender', 'Employment_Status']\n",
    "\n",
    "df = replace_yesna(df, demo,'Prefer not to say')\n",
    "\n",
    "# fill Age with '25-34 years old'\n",
    "df = replace_yesna(df, 'Age','25-34 years old')\n",
    "\n",
    "# fill 'OpSysPersonal use' & 'OpSysProfessional use' with 'Windows'\n",
    "ops = ['OpSysPersonal_use', 'OpSysProfessional_use']\n",
    "df = replace_yesna(df, ops,'Windows')\n",
    "\n",
    "# fill  SOVisitFreq with 'Daily or almost daily'\n",
    "df = replace_yesna(df, 'SOVisitFreq','Daily or almost daily')\n",
    "\n",
    "# fill Blockchain_Sentiment with 'Unsure'\n",
    "df = replace_yesna(df, 'Blockchain_Sentiment','Unsure')\n",
    "\n",
    "# fill Coding_Experience, Pro_Experience with median\n",
    "df['Coding_Experience'].fillna(df['Coding_Experience'].median(), inplace=True)\n",
    "df['Pro_Experience'].fillna(df['Pro_Experience'].median(), inplace=True)\n",
    "\n",
    "# fill VersionControlSystem with 'I don't use one'\n",
    "df = replace_yesna(df, 'VersionControlSystem',\"I don't use one\")\n",
    "\n",
    "# fill Belong_SO_Comm with 'Not sure'\n",
    "df = replace_yesna(df, 'Belong_SO_Comm',\"Not sure\")\n",
    "\n",
    "# fill Education_Level with 'Something else'\n",
    "df = replace_yesna(df, 'Education_Level',\"Something else\")\n",
    "\n",
    "# fill SOAccount_Ownership with Not sure/can't remember\n",
    "df = replace_yesna(df, 'SOAccount_Ownership',\"Not sure/can't remember\")\n",
    "\n",
    "# fill Learn_Code_Method with 'I don't use one'\n",
    "df = replace_yesna(df, 'Learn_Code_Method',\"I don't use one\")\n",
    "\n",
    "# fill SOA_ParticipationFreq with 'Less than once per month or monthly'\n",
    "df = replace_yesna(df, 'SOA_ParticipationFreq',\"Less than once per month or monthly\")\n",
    "\n",
    "# fill LanguageHaveWorkedWith with 'Bash/Shell'\n",
    "df = replace_yesna(df, 'LanguageHaveWorkedWith',\"Bash/Shell\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6d2c05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(describe_df.missing_values()))\n",
    "describe_df.missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5cf266",
   "metadata": {},
   "source": [
    "#### Handling Multiple Response Values\n",
    "From the profiles analysis, the columns below were discovered to contain multiple and fragmented respones for each row, which masked the true distribution across categories due to unique combination of values contained in each row. To untangle the values and have unique responses, we will select the first mentioned response. Our rationale is that this responses for e.g in `Code_Certifications` represents the online resource that was used most often, was most impactful, or most memorable to the developer. The same arguement holds for the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "06bc5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to extract first mentions\n",
    "def extract_first_value_from_columns(df, columns):\n",
    "    first_values = {}\n",
    "    for col in columns:\n",
    "        first_values[col] = df[col].apply(lambda x: x.split(\";\")[0].strip() if isinstance(x, str) and \";\" in x else x)\n",
    "    return pd.DataFrame(first_values)\n",
    "\n",
    "# columns to be cleaned\n",
    "colslist = ['Code_Certifications', 'Proj_Mgmt_WantToWorkWith', 'MiscTechWantToWorkWith', 'PlatformWantToWorkWith', \n",
    "'WebframeWantToWorkWith', 'Proj_Mgmt_HVWorkedWith', 'ToolsTechWantToWorkWith', 'Proj_Mgmt_SyncWantToWorkWith', \n",
    "'PlatformHaveWorkedWith', 'Learn_Code_Source', 'DatabaseWantToWorkWith', 'WebframeHaveWorkedWith', \n",
    "'ToolsTechHaveWorkedWith', 'Outofwork_Coding', 'DatabaseHaveWorkedWith', 'DevType', \n",
    "'Proj_Mgmt_SyncHaveWorkedWith', 'NEWCollabToolsWantToWorkWith', 'OpSysProfessional_use', 'MentalHealth', \n",
    "'Sexuality', 'LanguageWantToWorkWith', 'Disability', 'Purchase_Research', 'VCInteraction', 'Ethnicity', \n",
    "'NEWCollabToolsHaveWorkedWith', 'OpSysPersonal_use', 'LanguageHaveWorkedWith', 'New_Stack_Overfl_Sites', \n",
    "'VersionControlSystem', 'Learn_Code_Method', 'Employment_Status', 'Gender', 'ProfessionalTech']\n",
    "\n",
    "# data with cols of first mentions\n",
    "zer = extract_first_value_from_columns(df, colslist)\n",
    "zer.Code_Certifications.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "5a0c34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the colslist in df with cleaned data\n",
    "# from zer dataframe\n",
    "df[colslist] = zer\n",
    "\n",
    "# confirm that values display as expected\n",
    "df.Code_Certifications.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed2122",
   "metadata": {},
   "source": [
    "#### Cleaning the `Annual_Salary` Column\n",
    "To clean this column's missing values, we will look at:\n",
    "- the employment of each respondent. For unemployed and students without salary values, we will replace missing values with 0 as it is unrealistic to impute values for this group\n",
    "- the salary distribution at a regional level (e.g. salaries in Africa would differ from Europe), then use the appropriate median value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "c4f7913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for those who don't work and have no salary\n",
    "# fill missing salary with 0\n",
    "nonwork = ['Student, full-time', 'Student, part-time',\n",
    "       'Not employed, but looking for work',\n",
    " 'Not employed, and not looking for work',\n",
    "       'Retired', 'I prefer not to say']\n",
    "\n",
    "df.loc[(df['Employment_Status'].isin(nonwork)) & (df['Annual_Salary'].isna()), 'Annual_Salary'] = 0\n",
    "# confirm that the replace has worked\n",
    "print(df.Annual_Salary.value_counts().head(), '\\n')\n",
    "\n",
    "# confirm that other null values still exist\n",
    "print('remaining missing values: ', df.Annual_Salary.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "ad60ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries grouped into continents\n",
    "continents = {\n",
    "    'Asia': ['Israel', 'Hong Kong (S.A.R.)', 'India', 'China', 'Singapore', 'Iraq',\n",
    "             'Philippines', 'Iran, Islamic Republic of...', 'Indonesia', 'Afghanistan',\n",
    "             'Viet Nam', 'South Korea', 'Taiwan', 'Japan', 'Thailand', 'Bangladesh',\n",
    "             'Nepal', 'United Arab Emirates', 'Pakistan', 'Sri Lanka', 'Azerbaijan',\n",
    "             'Uzbekistan', 'Kazakhstan', 'North Korea', 'Timor-Leste', 'Brunei Darussalam',\n",
    "             'Oman', 'Saudi Arabia', 'Maldives', 'Jordan', 'Bahrain', 'Republic of Korea',\n",
    "            'Lebanon', 'Malaysia', \"Lao People's Democratic Republic\", 'Syrian Arab Republic',\n",
    "            'Qatar', 'Kyrgyzstan', 'Cambodia', 'Yemen', 'Mongolia', 'Tajikistan', 'Myanmar', \n",
    "            'Kuwait', 'Turkmenistan', 'Palestine', 'Bhutan'],\n",
    "    'Oceania': ['Australia', 'New Zealand', 'Fiji', 'Solomon Islands', 'Papua New Guinea', \n",
    "                'Palau'],\n",
    "    'North America': ['Canada', 'United States of America', 'Mexico', 'Dominican Republic', \n",
    "                     'Costa Rica', 'Nicaragua', 'Belize', 'Guatemala', 'El Salvador', \n",
    "                     'Jamaica', 'Cuba', 'Panama', 'Bahamas', 'Barbados', 'Antigua and Barbuda', \n",
    "                      'Haiti', 'Saint Lucia', 'Saint Kitts and Nevis'],\n",
    "    'Europe': ['Croatia', 'Netherlands', 'Czech Republic', 'Sweden', 'Denmark',\n",
    "               'Finland', 'United Kingdom of Great Britain and Northern Ireland',\n",
    "               'Austria', 'France', 'Portugal', 'Belgium', 'Ireland', 'Iceland',\n",
    "               'Montenegro', 'Germany', 'Belarus', 'Switzerland', 'Poland',\n",
    "               'Ukraine', 'Russia', 'Serbia', 'Luxembourg', 'Spain', 'Norway',\n",
    "               'Romania', 'Italy', 'Turkey', 'Greece', 'Hungary', 'Malta',\n",
    "               'Estonia', 'Slovenia', 'Bosnia and Herzegovina', 'Bulgaria',\n",
    "               'Georgia', 'Latvia', 'Lithuania', 'Moldova', 'Macedonia (FYROM)', 'Armenia',\n",
    "               'Monaco', 'Slovakia', 'Cyprus', 'Russian Federation', \n",
    "              'The former Yugoslav Republic of Macedonia', 'Andorra', 'Nomadic', 'Albania', \n",
    "              'Republic of Moldova', 'Kosovo', 'Isle of Man', 'San Marino'],\n",
    "    'Africa': ['Madagascar', 'South Africa', 'Swaziland', 'Mali', 'Egypt', 'Nigeria',\n",
    "               'Tunisia', 'Cameroon', 'Ethiopia', 'Ghana', 'Rwanda', 'Senegal',\n",
    "               'Chad', 'Benin', 'Angola', 'Namibia', 'Malawi', 'Sierra Leone',\n",
    "               'Zimbabwe', 'Mauritius', 'Morocco', 'Kenya', 'Botswana', 'Liberia', 'Lesotho', \n",
    "               'Guinea', 'Gabon', 'Seychelles', 'Algeria', 'Zambia', 'Uganda', \n",
    "              'United Republic of Tanzania', 'Niger', 'Cape Verde', 'Libyan Arab Jamahiriya', \n",
    "              'Togo', 'Sudan', 'Democratic Republic of the Congo', \"Côte d'Ivoire\", \n",
    "              'Congo, Republic of the...', 'Somalia', 'Mozambique', 'Mauritania', \n",
    "              'Burkina Faso', 'Gambia', 'Djibouti'],\n",
    "    'South America': ['Brazil', 'Argentina', 'Colombia', 'Chile', 'Peru',\n",
    "                      'Venezuela, Bolivarian Republic of...', 'Bolivia', 'Paraguay',\n",
    "                      'Ecuador', 'Uruguay', 'Honduras', 'Trinidad and Tobago', 'Suriname', \n",
    "                     'Guyana']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "14f6d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'continents' column by mapping the 'countries' column to the continents dictionary\n",
    "df['continents'] = df['Country'].map({country: continent for continent, countries in continents.items() \n",
    "                                        for country in countries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "43975d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get median salary for each continent\n",
    "salo = df[df.Annual_Salary > 0].groupby('continents')['Annual_Salary'].median()\n",
    "salo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "3a54bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "continent_mapping = {\n",
    "    'Africa': 18126.0,\n",
    "    'Asia': 23215.0,\n",
    "    'Europe': 59720.0,\n",
    "    'North America': 132000.0,\n",
    "    'Oceania': 92002.0,\n",
    "    'South America': 27008.5}\n",
    "\n",
    "df['Annual_Salary'] = df.groupby('continents')['Annual_Salary'].apply(lambda x: x.fillna(continent_mapping[x.name]))\n",
    "\n",
    "df.Annual_Salary.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "2c718f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview salary columnn after handling missing values\n",
    "df[['continents', 'Country', 'Annual_Salary']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "9a865fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm no more missing values\n",
    "describe_df.missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c63f14",
   "metadata": {},
   "source": [
    "# Outliers \n",
    "We have 1 numerical column Annual Salary. Below codes are checking for outliers \n",
    "* We have  455 outliers , the lowest Outlier in salary is : 1828416.0, the highest Outlier in salary is  : 50000000.0, we also have 0 salary values which are the highest in count.\n",
    "* Considering this is real world data we have log transformed the values and for the 0 values we have added a constant of 0.00001 to handle 0 values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "001bc451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Annual_Salary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c3c80152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annual salary distribution \n",
    "df.hist('Annual_Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f599a7",
   "metadata": {},
   "source": [
    "Below we are trying to see who the majority of 0 are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "3f49d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = df[df['Annual_Salary'] == 0]\n",
    "print(filtered_data.shape[0])\n",
    "filtered_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "70bf8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "avedf = df[(df[\"Employment_Status\"] == \"Student, full-time\") | (df[\"Employment_Status\"] == \"Student, part-time\") & \n",
    "           (df[\"Annual_Salary\"]!= 0.0)]\n",
    "avedf.Annual_Salary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "272c44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.groupby('continents').Employment_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "1f3beeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.continents.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "81675579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.continents.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "c999ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to check for Outliers \n",
    "def check_outliers(data):\n",
    "    \"\"\"A function to check for outliers in the numeric columns using Z-Score\"\"\"\n",
    "    series_num = data[\"Annual_Salary\"]\n",
    "    mean = np.mean(series_num)\n",
    "    std = np.std(series_num)\n",
    "    threshold = 3\n",
    "    outliers = data[np.abs((series_num - mean) / std) > threshold]\n",
    "    return outliers\n",
    "\n",
    "outliers = check_outliers(df)\n",
    "\n",
    "print(\"The Number of Outliers in the 'Annual_Salary' Column:\", len(outliers))\n",
    "print(\"Outliers:\")\n",
    "outliers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9e947853",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#boxplot to check outliers \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(df['Annual_Salary'])\n",
    "plt.ylabel('Salary')\n",
    "plt.yscale('log')\n",
    "plt.title('Boxplot of Salaries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "164a8319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of above \n",
    "print(\"We have \", len(outliers)) \n",
    "print(\"The lowest Outlier in salary is :\", outliers.Annual_Salary.min())\n",
    "print(\"The highest Outlier in salary is  :\", outliers.Annual_Salary.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132e450",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "We will explore a few varibales below to help us better understand our data:\n",
    "- **Developer demographics** : continent, years of experience - work, age, gender, coding, education level, , where learned coding, remote vs onsite\n",
    "- **Developer tools**: Language worked with, database worked with, cloud platforms worked with, version control, operating system\n",
    "- **Annual_Salary** - histogram, boxplot. salary by continent, by educational level, years of experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860cc8de",
   "metadata": {},
   "source": [
    "#### Developer Distribution by Continent\n",
    "Most of the data was gathered from developers in Europe, North America and Asia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "fdc44444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt developer distribution by continent\n",
    "con = round(df.continents.value_counts(normalize=True), 2)\n",
    "plt.bar(con.index, con.values)\n",
    "plt.title('Developer Distribution by Continent')\n",
    "for i, v in enumerate(con.values):\n",
    "    plt.text(i, v, str(v), ha='center', va='bottom')\n",
    "plt.xlabel('continents')\n",
    "plt.ylabel('% count');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3357eed",
   "metadata": {},
   "source": [
    "#### Years of Experience : Professionnal & Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "9b0ef196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert cols from 'object' to 'float' type\n",
    "df['Pro_Experience'] = df.Pro_Experience.astype('float')\n",
    "df['Coding_Experience'] = df.Coding_Experience.astype('float')\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "fig.suptitle('Years of Experience : Professionnal & Coding')\n",
    "axes[0].hist(df.Pro_Experience)\n",
    "axes[0].axvline(df.Pro_Experience.median(), c='r')\n",
    "axes[0].set_xlabel('Years of Professional Experience')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[1].hist(df.Coding_Experience)\n",
    "axes[1].axvline(df.Coding_Experience.median(), c='r')\n",
    "axes[1].set_xlabel('Years of Coding Experience')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.legend(['median', 'median']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180e926",
   "metadata": {},
   "source": [
    "#### Age and Gender Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "4d0fa6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "genl = ['In your own words:',\n",
    "       'LBGTQIA',\n",
    "       'I prefer not to say', 'Prefer not to say', 'Woman', 'Man']\n",
    "\n",
    "#\n",
    "agedf = df.Age.value_counts().sort_values(ascending=True)\n",
    "genddf = df.Gender.value_counts().sort_values(ascending=True)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(18,6))\n",
    "\n",
    "fig.suptitle('Age & Gender')\n",
    "axes[0].barh(agedf.index, agedf.values)\n",
    "axes[0].set_ylabel('Age')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[1].barh(genddf.index, genddf.values)\n",
    "axes[1].set_ylabel('Gender')\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_yticks(range(len(genl)))\n",
    "axes[1].set_yticklabels(genl);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6bdbf",
   "metadata": {},
   "source": [
    "#### Source of Coding Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "b388c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, figsize=(4,25))\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, y='Learn_Code_Method', order=df['Learn_Code_Method'].value_counts().index, palette='viridis', ax=axes[0])\n",
    "axes[0].set_ylabel('Learn Code Method')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_title('Source of Coding Knowhow ')\n",
    "\n",
    "sns.countplot(data=df, y='Learn_Code_Source', order=df['Learn_Code_Source'].value_counts().index, palette='viridis', ax=axes[1])\n",
    "axes[1].set_ylabel('Learn Code Source')\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_title('Source of Coding Skill ')\n",
    "\n",
    "sns.countplot(data=df, y='Code_Certifications', order=df['Code_Certifications'].value_counts().index, palette='viridis', ax=axes[2])\n",
    "axes[2].set_ylabel('Coding Certs')\n",
    "axes[2].set_xlabel('Count')\n",
    "axes[2].set_title('Coding Certs ')\n",
    "\n",
    "sns.countplot(data=df, y='Education_Level', order=df['Education_Level'].value_counts().index, palette='viridis', ax=axes[3])\n",
    "axes[3].set_ylabel('Education Level')\n",
    "axes[3].set_xlabel('Count')\n",
    "axes[3].set_title('Education Level');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0efa05",
   "metadata": {},
   "source": [
    "#### Employment Status & Work Style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "90824ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "empl = ['Retired', 'I prefer not to say',\n",
    "       'Not employed, not searching', 'Employed, part-time',\n",
    "       'Not employed, searching', 'Student, part-time',\n",
    "       'consultant/freelance',\n",
    "       'Student, full-time', 'Employed, full-time']\n",
    "\n",
    "wkstlls = ['Full in-person', 'None',\n",
    "       'Hybrid', 'Fully remote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "e5412f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = df.Employment_Status.value_counts().sort_values(ascending=True)\n",
    "wkstl = df.Remote_vs_Onsite.value_counts().sort_values(ascending=True)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(15,6))\n",
    "\n",
    "fig.suptitle('Employment Status & Work Style')\n",
    "axes[0].barh(emp.index, emp.values)\n",
    "axes[0].set_ylabel('Employment Status')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_yticks(range(len(empl)))\n",
    "axes[0].set_yticklabels(empl)\n",
    "axes[1].barh(wkstl.index, wkstl.values)\n",
    "axes[1].set_ylabel('Work Style')\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_yticks(range(len(wkstlls)))\n",
    "axes[1].set_yticklabels(wkstlls);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c957fd9",
   "metadata": {},
   "source": [
    "#### Annual Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc79e789",
   "metadata": {},
   "source": [
    "#### Salary Distribution by Continent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e31ff",
   "metadata": {},
   "source": [
    "North American developers are the most well-paid, followed by Oceania and Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "322cf76f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "salo_sorted = salo.sort_values(ascending=True)\n",
    "plt.figure(figsize=(6, 3))\n",
    "salo_sorted.plot(kind='barh')\n",
    "plt.ylabel('Continents')\n",
    "plt.xlabel('Median Annual Salary')\n",
    "plt.title('Median Annual Salary by Continents)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "c34eda52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "ax = sns.boxplot(x='continents', y='Annual_Salary', data=df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Continent')\n",
    "plt.ylabel('Annual Developer Salary')\n",
    "plt.title('Annual Developer Salaries by Continent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5dbfa1",
   "metadata": {},
   "source": [
    "#### Developer Tools By Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "a016066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots with 4 rows\n",
    "fig, axes = plt.subplots(nrows=4, figsize=(22, 30))\n",
    "# Define the columns to plot\n",
    "columns_to_plot = ['LanguageHaveWorkedWith', 'DatabaseHaveWorkedWith', 'PlatformHaveWorkedWith', 'WebframeHaveWorkedWith']\n",
    "# Iterate over the columns and create plots\n",
    "for i, column in enumerate(columns_to_plot):\n",
    "    # Group the data by column and calculate the average annual salary\n",
    "    median_salary_by_column = df.groupby(column)['Annual_Salary'].median().reset_index()\n",
    "    median_salary_by_column = median_salary_by_column.sort_values('Annual_Salary', ascending=False)\n",
    "    top_10_categories = median_salary_by_column.head(10)\n",
    "    # Create the bar plot for each column\n",
    "    axes[i].bar(top_10_categories[column], top_10_categories['Annual_Salary'])\n",
    "    axes[i].set_xlabel(column)\n",
    "    axes[i].set_ylabel('Median Annual Salary')\n",
    "    axes[i].set_title(f'Top 10 {column} by Median Annual Salary')\n",
    "    #axes[i].tick_params(axis='x', rotation=45, rotation_mode='anchor', ha='right')\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "fb5f0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by VersionControlSystem and calculate the median annual salary\n",
    "average_salary_by_version_control = df.groupby('VersionControlSystem')['Annual_Salary'].median().reset_index()\n",
    "average_salary_by_version_control = average_salary_by_version_control.sort_values('Annual_Salary', ascending=True)\n",
    "# Create the vertical bar plot for VersionControlSystem\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.barh(average_salary_by_version_control['VersionControlSystem'], average_salary_by_version_control['Annual_Salary'])\n",
    "plt.ylabel('Version Control System')\n",
    "plt.xlabel('Average Annual Salary')\n",
    "plt.title('Version Control Systems by Average Annual Salary')\n",
    "plt.xticks(rotation=0, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd3690",
   "metadata": {},
   "source": [
    "## Multivariate Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "a3b19a67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Subset the relevant columns\n",
    "subset_df = df[[\"LanguageHaveWorkedWith\", \"Annual_Salary\", \"Education_Level\"]]\n",
    "\n",
    "# Filter out rows with missing values in any of the selected columns\n",
    "subset_df = subset_df.dropna(subset=[\"LanguageHaveWorkedWith\", \"Annual_Salary\", \"Education_Level\"])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=subset_df, x=\"LanguageHaveWorkedWith\", y=\"Annual_Salary\", hue=\"Education_Level\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Languages Worked With\")\n",
    "plt.ylabel(\"Annual Salary\")\n",
    "plt.title(\"Relationship between Languages, Salary, and Education Level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f39976",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b685ebb",
   "metadata": {},
   "source": [
    "### Data Encoding\n",
    "Our data mostly contains categorical variables that need to be preprocessed via encoding to make it ready for modelling\n",
    "Before encoding the data, based on domain knowledge, we will select the required features that are most likely to affect `Annual_Salary`. Below, `deletedf` (38 columns) shows the list of columns to be dropped, while `selectorf` (36 columns), shows the columns to be used in preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "89edad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "deletedf = ['LanguageWantToWorkWith', 'DatabaseWantToWorkWith', 'PlatformWantToWorkWith', 'WebframeWantToWorkWith', \n",
    "'MiscTechWantToWorkWith', 'ToolsTechWantToWorkWith', 'NEWCollabToolsWantToWorkWith', 'Proj_Mgmt_WantToWorkWith', \n",
    "'Proj_Mgmt_SyncWantToWorkWith', 'Blockchain_Sentiment', 'New_Stack_Overfl_Sites', 'SOVisitFreq', 'SOAccount_Ownership', \n",
    "'SOA_ParticipationFreq', 'Belong_SO_Comm', 'Participation_PDS', 'Contributor_or_Manager', 'Extroverted', 'Siloed_Teams', \n",
    "'Information_Availability', 'Well_Resourced', 'Info_Answ_Resource', 'Recurring_Responses', 'Workflow_Interruptions', \n",
    "'External_Team_Assistance', 'External_Team_Interaction', 'Knowledge_Silos', 'TimeSearching_Answers', 'TimeAnswering', \n",
    "'Onboarding_Duration', 'ProfessionalTech', 'Is_Involved_Onboarding', 'Employer_Learning_Resources', 'Learning_Support', \n",
    "'SurveyLength', 'SurveyEase', 'Purchase_Research']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "8bba6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectorf = ['Developer_Description', 'Employment_Status',\n",
    "       'Remote_vs_Onsite', 'Outofwork_Coding', 'Education_Level',\n",
    "       'Learn_Code_Method', 'Learn_Code_Source', 'Code_Certifications',\n",
    "       'Coding_Experience', 'Pro_Experience', 'DevType', 'OrgSize',\n",
    "       'PurchaseInfluence', 'Country',\n",
    "       'LanguageHaveWorkedWith',\n",
    "       'DatabaseHaveWorkedWith',\n",
    "       'PlatformHaveWorkedWith',\n",
    "       'WebframeHaveWorkedWith', \n",
    "       'MiscTechHaveWorkedWith',\n",
    "       'ToolsTechHaveWorkedWith',\n",
    "       'NEWCollabToolsHaveWorkedWith',\n",
    "       'OpSysProfessional_use', 'OpSysPersonal_use', 'VersionControlSystem',\n",
    "       'VCInteraction', 'Proj_Mgmt_HVWorkedWith',\n",
    "       'Proj_Mgmt_SyncHaveWorkedWith', \n",
    "       'Age',\n",
    "       'Gender', 'Trans', 'Sexuality', 'Ethnicity', 'Disability',\n",
    "       'MentalHealth', 'Annual_Salary',\n",
    "       'continents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "2974161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(deletedf), len(selectorf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8fbf22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_binary_encode = ['Employment_Status', 'ProfessionalTech', 'OpSysPersonal_use', 'OpSysProfessional_use', \n",
    "                            'Remote_vs_Onsite','Developer_Description', 'Outofwork_Coding', 'Learn_Code_Method', \n",
    "                            'Learn_Code_Source', 'Code_Certifications', 'DevType', 'Purchase_Research', 'Country', \n",
    "                            'LanguageHaveWorkedWith', 'DatabaseHaveWorkedWith', 'LanguageWantToWorkWith', \n",
    "                            'DatabaseWantToWorkWith', 'PlatformHaveWorkedWith', 'PlatformWantToWorkWith', \n",
    "                            'WebframeHaveWorkedWith', 'WebframeWantToWorkWith', 'MiscTechHaveWorkedWith', \n",
    "                            'MiscTechWantToWorkWith', 'ToolsTechHaveWorkedWith', 'ToolsTechWantToWorkWith', \n",
    "                            'NEWCollabToolsHaveWorkedWith', 'NEWCollabToolsWantToWorkWith', 'VersionControlSystem', \n",
    "                            'VCInteraction', 'Proj_Mgmt_HVWorkedWith', 'Proj_Mgmt_WantToWorkWith', \n",
    "                            'Proj_Mgmt_SyncHaveWorkedWith', 'Proj_Mgmt_SyncWantToWorkWith', 'New_Stack_Overfl_Sites', \n",
    "                            'SOAccount_Ownership', 'Belong_SO_Comm', 'Gender', 'Trans', 'Sexuality', 'Ethnicity', \n",
    "                            'Disability', 'MentalHealth', 'Participation_PDS', 'Contributor_or_Manager', \n",
    "                            'Is_Involved_Onboarding', 'Employer_Learning_Resources', 'Learning_Support', 'continents' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "6568b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_ordinal_encode = ['Education_Level', 'OrgSize', 'PurchaseInfluence', 'Blockchain_Sentiment', 'SOVisitFreq', \n",
    "                             'SOA_ParticipationFreq', 'Age', 'Extroverted', 'Siloed_Teams', 'Information_Availability', \n",
    "                             'Well_Resourced', 'Info_Answ_Resource', 'Recurring_Responses', 'Workflow_Interruptions', \n",
    "                             'External_Team_Assistance', 'External_Team_Interaction', 'Knowledge_Silos', \n",
    "                             'TimeSearching_Answers', 'TimeAnswering', 'Onboarding_Duration', 'SurveyLength', 'SurveyEase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "e47b9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['Coding_Experience', 'Pro_Experience','Annual_Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "c638d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_binary_encode_set = set(columns_to_binary_encode)\n",
    "columns_to_ordinal_encode_set = set(columns_to_ordinal_encode)\n",
    "deletedf_set = set(deletedf)\n",
    "\n",
    "final_to_binary_encode = list(columns_to_binary_encode_set - deletedf_set)\n",
    "final_to_ordinal_encode = list(columns_to_ordinal_encode_set - deletedf_set)\n",
    "\n",
    "print(len(final_to_binary_encode))\n",
    "print(len(final_to_ordinal_encode))\n",
    "print(len(selectorf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "c1a10665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final cols to work with\n",
    "#final_to_binary_encode\n",
    "#final_to_ordinal_encode\n",
    "#numeric_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "d71c45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop work experience as its correlated to \n",
    "# professional experience, hence duplication\n",
    "df = dropping_columns(df, 'WorkExp')\n",
    "df = dropping_columns(df, 'ResponseId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "30a9bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform binary encoding\n",
    "def perform_binary_encoding(df, columns):\n",
    "    # Create a copy of the original DataFrame\n",
    "    df_encoded = df.copy()\n",
    "    # Perform Binary Encoding for each specified column\n",
    "    for column in columns:\n",
    "        binary_encoder = ce.BinaryEncoder(cols=[column])\n",
    "        df_encoded = binary_encoder.fit_transform(df_encoded)\n",
    "    return df_encoded\n",
    "# Perform Binary Encoding\n",
    "df_encoded_binary = perform_binary_encoding(df, final_to_binary_encode)\n",
    "# Display the encoded dataframe\n",
    "df_encoded_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "58f62da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform ordinal encoding\n",
    "def perform_ordinal_encoding(df, columns):\n",
    "    # Create a copy of the original DataFrame\n",
    "    df_encod = df.copy()\n",
    "    # Perform Ordinal Encoding for each specified column\n",
    "    for column in columns:\n",
    "        ordinal_encoder = ce.OrdinalEncoder(cols=[column])\n",
    "        df_encod[column] = ordinal_encoder.fit_transform(df_encod[column])\n",
    "    return df_encod\n",
    "# Perform Ordinal Encoding\n",
    "df_encod = perform_ordinal_encoding(df_encoded_binary, final_to_ordinal_encode)\n",
    "# Display the encoded dataframe\n",
    "df_encod.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "bf917714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop deleted columns\n",
    "df_encod = dropping_columns(df_encod, deletedf)\n",
    "\n",
    "# confirm no categorical variables\n",
    "print(df_encod.select_dtypes(include='object').columns)\n",
    "df_encod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "a94164a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Handling Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1456fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "Outlier Detection and Handling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
